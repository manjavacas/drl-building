> 锔 El entorno elaborado en este trabajo y previamente denominado _Energym_ posteriormente pas贸 a renombrarse como **Sinergym**.

**Trabajo Fin de M谩ster. M谩ster Universitario Oficial en Ciencia de Datos e Ingenier铆a de Computadores**

**Antonio Manjavacas Lucas**

Curso 2020-2021

Universidad de Granada

# Deep Reinforcement Learning para control energ茅tico eficiente de edificios

## Resumen 

En las 煤ltimas d茅cadas, tanto el calentamiento global como el cambio clim谩tico se han visto significativamente alentados por la demanda energ茅tica de edificios residenciales y comerciales. Estos son responsables de un tercio del consumo mundial de energ铆a y de hasta un 40\% de las emisiones de CO2, mayormente producidas por los sistemas de calefacci贸n, ventilaci贸n y aire acondicionado (HVAC) destinados a garantizar el bienestar de sus ocupantes.

Ante esta problem谩tica, optimizar el control de los sistemas HVAC se plantea como una soluci贸n necesaria ante el creciente inter茅s por garantizar la eficiencia energ茅tica de los edificios. Dicho control ha sido tradicionalmente llevado a cabo mediante t茅cnicas basadas en modelos de predicci贸n, los cuales no siempre garantizan la maximizaci贸n del confort de los ocupantes y, al mismo tiempo, la minimizaci贸n del consumo energ茅tico.

En contraposici贸n a estos m茅todos tradicionales, en los 煤ltimos a帽os se ha experimentado una notable tendencia al uso de t茅cnicas basadas en aprendizaje profundo por refuerzo (DRL) orientadas a control HVAC, logrando mejorar los resultados ofrecidos por m茅todos de control convencionales. No obstante, se trata de un campo relativamente inmaduro, donde se carece de marcos de referencia y bancos de prueba espec铆ficamente destinados a reproducir y comparar los diferentes algoritmos que conforman el estado del arte.

En respuesta a esta necesidad, el objetivo perseguido en este trabajo ser谩 el desarrollo de un entorno de ejecuci贸n de simulaciones energ茅ticas orientado al uso y evaluaci贸n de diferentes algoritmos de DRL en control HVAC. A su vez, se profundizar谩 en la experimentaci贸n con estos algoritmos haciendo uso del entorno implementado, evaluando los resultados obtenidos en t茅rminos de consumo energ茅tico y confort.

## Gu铆a del repositorio 

* [agents](https://github.com/manjavacas/drl-building/tree/main/agents): _scripts_ destinados al entrenamiento y ejecuci贸n de los agentes empleados. Estos son:
    - [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html): _Advantage Actor Critic_.
    - [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html): _Proximal Policy Optimization_.
    - [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html): _Deep Q-Networks_.
    - [DDPG](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html): _Deep Deterministic Policy Gradient_.
    - [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html): _Soft Actor Critic_.
    - RBC: controlador basado en reglas.
    - RAND: agente aleatorio.
* [mlruns](https://github.com/manjavacas/drl-building/tree/main/mlruns/0): historial de ejecuciones registrado por [MLflow](https://mlflow.org/).
* [models](https://github.com/manjavacas/drl-building/tree/main/models): modelos entrenados.
* [plots](https://github.com/manjavacas/drl-building/tree/main/plots): datos y gr谩ficos generados a partir de los resultados de las simulaciones.
* [tensorboard_logs](https://github.com/manjavacas/drl-building/tree/main/tensorboard_log): _logs_ registrados durante los entrenamientos y empleados por [TensorBoard](https://www.tensorflow.org/tensorboard).
* [mem](https://github.com/manjavacas/drl-building/tree/main/mem): memoria del proyecto.

## Desarrollo de ~Energym~ **Sinergym** 

Los resultados han sido obtenidos a partir del entrenamiento y ejecuci贸n de diferentes agentes en el entorno de ejecuci贸n de simulaciones energ茅ticas ~Energym~ [Sinergym](https://github.com/jajimer/energym) (versi贸n 1.0.0), elaborado a lo largo de este TFM. Pulsa [aqu铆](https://energym.readthedocs.io/) para acceder a la documentaci贸n de ~Energym~ Sinergym.

![Arquitectura de Energym](/images/energym_diagram.png)

## Agradecimientos 

Gracias a Juan G贸mez, Miguel Molina, Javier Jim茅nez y Alejandro Campoy por su implicaci贸n y supervisi贸n a lo largo del desarrollo de este proyecto. 

Este trabajo se enmarca dentro del proyecto [PROFICIENT](https://jgromero.github.io/proficient/), financiado por el programa EXPLORA del Ministerio de Ciencia, Innovaci贸n y Universidades (TIN2017-91223-EXP) y orientado al desarrollo de soluciones basadas en DRL para el control energ茅tico eficiente de edificios.
