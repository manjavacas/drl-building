**Trabajo Fin de M√°ster. M√°ster Universitario Oficial en Ciencia de Datos e Ingenier√≠a de Computadores**

[Antonio Manjavacas Lucas](<manjavacas@correo.ugr.es>)

Curso 2020-2021

Universidad de Granada

# Deep Reinforcement Learning para control energ√©tico eficiente de edificios

## Resumen üìñ

En las √∫ltimas d√©cadas, tanto el calentamiento global como el cambio clim√°tico se han visto significativamente alentados por la demanda energ√©tica de edificios residenciales y comerciales. Estos son responsables de un tercio del consumo mundial de energ√≠a y de hasta un 40\% de las emisiones de CO2, mayormente producidas por los sistemas de calefacci√≥n, ventilaci√≥n y aire acondicionado (HVAC) destinados a garantizar el bienestar de sus ocupantes.

Ante esta problem√°tica, optimizar el control de los sistemas HVAC se plantea como una soluci√≥n necesaria ante el creciente inter√©s por garantizar la eficiencia energ√©tica de los edificios. Dicho control ha sido tradicionalmente llevado a cabo mediante t√©cnicas basadas en modelos de predicci√≥n, los cuales no siempre garantizan la maximizaci√≥n del confort de los ocupantes y, al mismo tiempo, la minimizaci√≥n del consumo energ√©tico.

En contraposici√≥n a estos m√©todos tradicionales, en los √∫ltimos a√±os se ha experimentado una notable tendencia al uso de t√©cnicas basadas en aprendizaje profundo por refuerzo (DRL) orientadas a control HVAC, logrando mejorar los resultados ofrecidos por m√©todos de control convencionales. No obstante, se trata de un campo relativamente inmaduro, donde se carece de marcos de referencia y bancos de prueba espec√≠ficamente destinados a reproducir y comparar los diferentes algoritmos que conforman el estado del arte.

En respuesta a esta necesidad, el objetivo perseguido en este trabajo ser√° el desarrollo de un entorno de ejecuci√≥n de simulaciones energ√©ticas orientado al uso y evaluaci√≥n de diferentes algoritmos de DRL en control HVAC. A su vez, se profundizar√° en la experimentaci√≥n con estos algoritmos haciendo uso del entorno implementado, evaluando los resultados obtenidos en t√©rminos de consumo energ√©tico y confort.

## Gu√≠a del repositorio üîé

* [agents](https://github.com/manjavacas/drl-building/tree/main/agents): _scripts_ destinados al entrenamiento y ejecuci√≥n de los agentes empleados. Estos son:
    - [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html): _Advantage Actor Critic_.
    - [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html): _Proximal Policy Optimization_.
    - [DQN](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html): _Deep Q-Networks_.
    - [DDPG](https://stable-baselines3.readthedocs.io/en/master/modules/ddpg.html): _Deep Deterministic Policy Gradient_.
    - [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html): _Soft Actor Critic_.
    - RBC: controlador basado en reglas.
    - RAND: agente aleatorio.
* [mlruns](https://github.com/manjavacas/drl-building/tree/main/mlruns/0): historial de ejecuciones registrado por [MLflow](https://mlflow.org/).
* [models](https://github.com/manjavacas/drl-building/tree/main/models): modelos entrenados.
* [plots](https://github.com/manjavacas/drl-building/tree/main/plots): datos y gr√°ficos generados a partir de los resultados de las simulaciones.
* [tensorboard_logs](https://github.com/manjavacas/drl-building/tree/main/tensorboard_log): _logs_ registrados durante los entrenamientos y empleados por [TensorBoard](https://www.tensorflow.org/tensorboard).
* [mem](https://github.com/manjavacas/drl-building/tree/main/mem): memoria del proyecto.

## Desarrollo de Energym üí°

Los resultados han sido obtenidos a partir del entrenamiento y ejecuci√≥n de diferentes agentes en el entorno de ejecuci√≥n de simulaciones energ√©ticas [Energym](https://github.com/jajimer/energym) (versi√≥n 1.0.0), elaborado a lo largo de este TFM. Pulsa [aqu√≠](https://energym.readthedocs.io/) para acceder a la documentaci√≥n de Energym.

![Arquitectura de Energym](/images/energym_diagram.png)

## Agradecimientos üéÅ

Gracias a Juan G√≥mez, Miguel Molina, Javier Jim√©nez y Alejandro Campoy por su implicaci√≥n y supervisi√≥n a lo largo del desarrollo de este proyecto. 

Este trabajo se enmarca dentro del proyecto [PROFICIENT](https://jgromero.github.io/proficient/), financiado por el programa EXPLORA del Ministerio de Ciencia, Innovaci√≥n y Universidades (TIN2017-91223-EXP) y orientado al desarrollo de soluciones basadas en DRL para el control energ√©tico eficiente de edificios.